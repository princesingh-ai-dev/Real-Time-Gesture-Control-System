# âœ‹ Real-Time Gesture Control System

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![MediaPipe](https://img.shields.io/badge/MediaPipe-4285F4?style=for-the-badge&logo=google&logoColor=white)
![OpenCV](https://img.shields.io/badge/OpenCV-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)

**Control your computer hands-free using gesture recognition â€” built with MediaPipe, SVM & OpenCV.**

A real-time hand gesture recognition system that maps gestures to keyboard/mouse commands for zero-touch computer control.

</div>

---

## âœ¨ Features

- ğŸ–ï¸ **5 Predefined Gestures** â€” Open palm, closed fist, thumbs up, pointing index, peace sign
- âš¡ **Low Latency** â€” Sub-100ms gesture recognition for seamless interaction
- ğŸ¯ **High Accuracy** â€” SVM classifier with configurable confidence threshold (default: 80%)
- ğŸ”§ **Customizable Mappings** â€” JSON-based gesture-to-command configuration
- ğŸ“Š **Visual Feedback** â€” Real-time confidence display and gesture indicators
- ğŸ“ **Built-in Training** â€” Collect your own gesture samples and train the model

## ğŸ¬ How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Webcam     â”‚â”€â”€â”€â”€â–¶â”‚ MediaPipe Hands  â”‚â”€â”€â”€â”€â–¶â”‚ SVM Classifierâ”‚â”€â”€â”€â”€â–¶â”‚   Execute    â”‚
â”‚   Feed       â”‚     â”‚ (21 Landmarks)   â”‚     â”‚ (scikit-learn)â”‚     â”‚   Command    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                        â”‚
                              â–¼                        â–¼
                     Feature Extraction        Confidence > 80%?
                     (x, y coordinates)        â”œâ”€â”€ Yes â†’ Execute
                                               â””â”€â”€ No  â†’ Ignore
```

## ğŸ® Default Gesture Mappings

| Gesture | Command | Description |
|---------|---------|-------------|
| ğŸ–ï¸ Open Palm | Volume Up | Raise hand to increase volume |
| âœŠ Closed Fist | Volume Down | Close fist to decrease volume |
| ğŸ‘ Thumbs Up | Mute/Unmute | Toggle audio mute |
| ğŸ‘† Pointing Index | Page Down | Scroll down through content |
| âœŒï¸ Peace Sign | Browser Back | Navigate to previous page |

## ğŸ“‹ Requirements

- Python 3.8+
- Webcam
- Windows / Linux / macOS

## ğŸš€ Quick Start

```bash
# 1. Clone the repository
git clone https://github.com/princesingh1702/Real-Time-Gesture-Control-System.git
cd Real-Time-Gesture-Control-System

# 2. Create virtual environment
python -m venv gesture_env
gesture_env\Scripts\activate      # Windows
# source gesture_env/bin/activate  # Linux/macOS

# 3. Install dependencies
pip install -r requirements.txt

# 4. Train the gesture model (first time only)
python train_model.py
# Follow on-screen instructions â€” collect ~100 samples per gesture

# 5. Run gesture control
python main.py
# Press 'q' to quit
```

## âš™ï¸ Configuration

Edit `config.json` to customize gesture mappings:

```json
{
  "gestures": {
    "open_palm": "volume_up",
    "closed_fist": "volume_down",
    "thumbs_up": "mute"
  },
  "confidence_threshold": 0.8
}
```

## ğŸ“ Project Structure

```
Real-Time-Gesture-Control-System/
â”œâ”€â”€ main.py            # Main application â€” webcam + gesture detection
â”œâ”€â”€ train_model.py     # Gesture data collection & SVM training
â”œâ”€â”€ config.json        # Gesture-to-command mappings
â”œâ”€â”€ requirements.txt   # Python dependencies
â””â”€â”€ gesture_model.pkl  # Trained model (generated after training)
```

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|------------|
| Hand Tracking | MediaPipe Hands (21 landmarks) |
| Classification | scikit-learn SVM |
| Computer Vision | OpenCV |
| System Control | PyAutoGUI |
| Math | NumPy |

## ğŸ“Š Performance

| Metric | Value |
|--------|-------|
| Recognition Latency | < 100ms |
| Training Accuracy | 95%+ (with 100+ samples/gesture) |
| Supported Gestures | 5 (expandable) |
| Frame Rate | 30 FPS |

## ğŸ› Troubleshooting

| Issue | Solution |
|-------|----------|
| Low accuracy | Collect more diverse samples, ensure good lighting |
| Webcam not detected | Close other apps using the camera, check device index |
| Commands not executing | Run as administrator (Windows) for PyAutoGUI permissions |
| Gesture not recognized | Adjust `confidence_threshold` in config.json |

## ğŸ“„ License

MIT License â€” Feel free to use and modify!